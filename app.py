from flask import Flask, jsonify
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re

app = Flask(__name__)

KEYWORDS = ["ESG ê²½ì˜", "ESG ìŠ¤íƒ€íŠ¸ì—…", "ì§€ì†ê°€ëŠ¥ì„±", "ESG ì§€ì›ì‚¬ì—…", "ESG í™œë™", "ESG ëª¨ì§‘"]
FILTER_WORDS = ["ëª¨ì§‘", "ì‹ ì²­"]

def summarize(text):
    for word in FILTER_WORDS:
        idx = text.find(word)
        if idx != -1:
            start = max(0, idx - 60)
            end = min(len(text), idx + 60)
            return '...' + text[start:end].strip() + '...'
    return text[:120] + '...'

@app.route("/esg_articles", methods=["GET"])
def fetch_esg_articles():
    yesterday = datetime.now() - timedelta(days=1)
    yesterday_str = yesterday.strftime("%Y.%m.%d")
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []

    for keyword in KEYWORDS:
        query = keyword.replace(" ", "+")
        search_url = f"https://search.naver.com/search.naver?where=news&query={query}&pd=2&ds={yesterday_str}&de={yesterday_str}"
        res = requests.get(search_url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        info_groups = soup.select("div.info_group")

        print(f"\nğŸ” [{keyword}] í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ê¸°ì‚¬ ìˆ˜: {len(info_groups)}")

        for info_group in info_groups:
            a_tag = info_group.select_one("a.info")
            date_span = info_group.select_one("span.info")

            if not a_tag or not date_span:
                continue

            href = a_tag.get("href")
            date_text = date_span.get_text()

            # ë‚ ì§œ í¬ë§· í™•ì¸
            if not re.match(r"\d{4}\.\d{2}\.\d{2}\.", date_text):
                print(f"â›” [{keyword}] {a_tag.get_text().strip()} â†’ ë‚ ì§œ ì •ë³´ ì—†ìŒ (â†’ {date_text})")
                continue
            if not date_text.startswith(yesterday_str):
                print(f"âš ï¸ [{keyword}] {a_tag.get_text().strip()} â†’ ì–´ì œ ë‚ ì§œ ì•„ë‹˜ ({date_text})")
                continue

            try:
                article = requests.get(href, headers=headers, timeout=5)
                article_soup = BeautifulSoup(article.text, "html.parser")
                text = article_soup.get_text()

                contains_kw = any(word in text for word in FILTER_WORDS)
                print(f"âœ… [{keyword}] {a_tag.get_text().strip()} â†’ ë³¸ë¬¸ ê¸¸ì´: {len(text)} | ëª¨ì§‘/ì‹ ì²­ í¬í•¨ ì—¬ë¶€: {contains_kw}")

                if contains_kw:
                    results.append({
                        "title": a_tag.get_text().strip(),
                        "link": href,
                        "summary": summarize(text)
                    })

            except Exception as e:
                print(f"âŒ [{keyword}] {a_tag.get_text().strip()} â†’ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue

    print(f"\nğŸ¯ ìµœì¢… í•„í„° í†µê³¼ ê¸°ì‚¬ ìˆ˜: {len(results)}")
    return jsonify(results)



if __name__ == '__main__':
    app.run(host='0.0.0.0', port=10000)
